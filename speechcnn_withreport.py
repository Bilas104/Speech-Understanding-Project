# -*- coding: utf-8 -*-
"""speechcnn_withreport.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tOrnpUGSYVJU4GlrZU0ewdjjeeLQloVz
"""

# ================================
# ‚öôÔ∏è Setup + GPU Check
# ================================

!pip install torch torchvision torchaudio librosa --quiet

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import librosa
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import joblib

from google.colab import drive
drive.mount('/content/drive')

# ‚úÖ Detect and use GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

BASE_PATH = '/content/drive/MyDrive/SpeechMajorProject_b'
MODEL_SAVE_PATH = os.path.join(BASE_PATH, 'deep_models')
os.makedirs(MODEL_SAVE_PATH, exist_ok=True)

# ================================
# üñºÔ∏è CNN Dataset: Spectrograms
# ================================

from feature_extraction import extract_features

from torchvision.transforms import ToTensor, Resize, Compose
from PIL import Image

class SpectrogramDataset(Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        path, label = self.dataset[idx]

        # Get image as NumPy array
        spec = extract_features(path, mode='spectrogram')  # returns np.array
        spec = spec / 255.0  # normalize

        # ‚úÖ Convert to PIL Image before applying transform
        image = Image.fromarray((spec * 255).astype(np.uint8))

        if self.transform:
            image = self.transform(image)

        label_idx = TARGET_EMOTIONS.index(label)
        return image, label_idx

# Transforms
from torchvision.transforms import ToTensor, Resize, Compose

transform = Compose([
    Resize((128, 128)),
    ToTensor()
])

# ================================
# üì¶ CNN Model (simple 3-layer)
# ================================

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=4):
        super(SimpleCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 16 * 16, 128), nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.conv(x)
        x = self.fc(x)
        return x

# ================================
# üß™ Training Loop
# ================================

from tqdm import tqdm

def train_model(model, train_loader, val_loader, num_epochs, model_name):
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        correct, total = 0, 0

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")

        for X, y in progress_bar:
            X, y = X.to(device), y.to(device)
            outputs = model(X)
            loss = criterion(outputs, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += y.size(0)
            correct += (predicted == y).sum().item()

            progress_bar.set_postfix(loss=loss.item(), acc=(correct / total))

        scheduler.step()

        # Save checkpoint
        if (epoch + 1) % 5 == 0:
            ckpt_path = os.path.join(MODEL_SAVE_PATH, f'{model_name}_epoch_{epoch+1}.pth')
            torch.save(model.state_dict(), ckpt_path)
            print(f"üíæ Saved checkpoint: {ckpt_path}")

    # Final model save
    final_path = os.path.join(MODEL_SAVE_PATH, f'{model_name}_final.pth')
    torch.save(model.state_dict(), final_path)
    print(f"‚úÖ Final model saved: {final_path}")

    # # Final save
    # torch.save(model.state_dict(), os.path.join(MODEL_SAVE_PATH, f'{model_name}_final.pth'))
    # print(f"‚úÖ Final model saved as {model_name}_final.pth")

def load_iemocap_data(root_dir, target_labels=None):
    import os
    dataset = []
    sessions = [os.path.join(root_dir, s) for s in os.listdir(root_dir) if s.startswith("Session")]

    for session in sessions:
        wav_root = os.path.join(session, "sentences/wav")  # ‚úÖ your actual structure
        label_folder = os.path.join(session, "dialog/EmoEvaluation")

        if not os.path.exists(label_folder):
            print(f"‚ö†Ô∏è Missing EmoEvaluation in {session}")
            continue

        for emo_file in os.listdir(label_folder):
            if emo_file.endswith(".txt"):
                emo_path = os.path.join(label_folder, emo_file)
                with open(emo_path, "r") as f:
                    lines = f.readlines()

                for line in lines:
                    if line.startswith('['):
                        parts = line.strip().split('\t')
                        if len(parts) >= 3:
                            filename = parts[1].strip()
                            emotion = parts[2].strip()

                            if (target_labels is None) or (emotion in target_labels):
                                # üîç Recursively search for the wav file
                                found = False
                                for root, _, files in os.walk(wav_root):
                                    if filename + '.wav' in files:
                                        full_path = os.path.join(root, filename + '.wav')
                                        dataset.append((full_path, emotion))
                                        found = True
                                        break
                                if not found:
                                    print(f"‚ö†Ô∏è File not found: {filename}.wav")

    print(f"‚úÖ Loaded {len(dataset)} labeled samples from IEMOCAP")
    return dataset


TARGET_EMOTIONS = ['hap', 'ang', 'sad', 'neu']  # Make sure it's consistent!
dataset = load_iemocap_data("/content/drive/MyDrive/SpeechMajorProject_b", target_labels=TARGET_EMOTIONS)

# ================================
# üë®‚Äçüç≥ Run CNN Training
# ================================



# Load full dataset list
# from preprocessing.load_dataset import load_iemocap_data
# dataset = load_iemocap_data(IEMOCAP_PATH, target_labels=TARGET_EMOTIONS)

train_set, val_set = train_test_split(dataset, test_size=0.2, stratify=[label for _, label in dataset])

train_dataset = SpectrogramDataset(train_set, transform=transform)
val_dataset = SpectrogramDataset(val_set, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

model = SimpleCNN(num_classes=len(TARGET_EMOTIONS))
model.to(device)
print(f"üì¶ Model is now on: {next(model.parameters()).device}")

train_model(model, train_loader, val_loader, num_epochs=5, model_name="cnn_spectrogram")

import torch

def save_model(model, path="cnn_model.pth"):
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

def load_model(model, path="cnn_model.pth", device="cpu"):
    model.load_state_dict(torch.load(path, map_location=device))
    model.to(device)
    model.eval()
    print(f"Model loaded from {path}")
    return model

torch.save(model.state_dict(), "cnn_model.pth")
print("Model saved to cnn_model.pth")

label_to_personality = {
    'hap': [0.8, 0.5, 0.9, 0.7, 0.2],  # O, C, E, A, N
    'sad': [0.4, 0.5, 0.2, 0.6, 0.9],
    'ang': [0.3, 0.4, 0.4, 0.3, 0.8],
    'neu': [0.5, 0.8, 0.6, 0.6, 0.3]
}

from torchvision.transforms import ToTensor, Resize, Compose
from PIL import Image
import torch
import numpy as np
import matplotlib.pyplot as plt
!pip install fpdf
from report_generator import plot_radar, generate_pdf

def predict_personality_from_audio(audio_path, model, transform, label_map):
    model.eval()

    # Extract spectrogram
    from feature_extraction import extract_features
    spec = extract_features(audio_path, mode='spectrogram')
    spec = spec / 255.0
    image = Image.fromarray((spec * 255).astype(np.uint8))
    if transform:
        image = transform(image)

    input_tensor = image.unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(input_tensor)
        predicted_index = torch.argmax(outputs, dim=1).item()
        predicted_label = TARGET_EMOTIONS[predicted_index]

    print(f"üéß Predicted Emotion: {predicted_label}")
    personality_vector = label_map[predicted_label]
    print(f"üß† Personality Traits (OCEAN): {personality_vector}")

    # Plot and save
    fig = plot_radar(torch.tensor(personality_vector), show=False)
    generate_pdf(personality_vector, fig, output_path="audio_personality_report.pdf")

sample_idx = 0

# Access the original path and label
sample_audio_path, emotion_label = val_set[sample_idx]

print(f"Audio path: {sample_audio_path}")
print(f"Actual emotion: {emotion_label}")

predict_personality_from_audio(sample_audio_path, model, transform, label_to_personality)

sample_id2= 560
sample_audio_path2, emotion_label2 = val_set[sample_idx]
print(f"Audio path: {sample_audio_path2}")
print(f"Actual emotion: {emotion_label2}")
predict_personality_from_audio(sample_audio_path2, model, transform, label_to_personality)

from torchvision.transforms import ToTensor, Resize, Compose
from PIL import Image
import torch
import numpy as np
import matplotlib.pyplot as plt
from report_generator import plot_radar, generate_pdf
from feature_extraction import extract_features

def predict_personality_from_audio(audio_path, model, transform, label_map, actual_label=None):
    model.eval()

    # Extract spectrogram
    spec = extract_features(audio_path, mode='spectrogram')
    spec = spec / 255.0
    image = Image.fromarray((spec * 255).astype(np.uint8))
    if transform:
        image = transform(image)

    input_tensor = image.unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(input_tensor)
        predicted_index = torch.argmax(outputs, dim=1).item()
        predicted_label = TARGET_EMOTIONS[predicted_index]

    predicted_traits = label_map[predicted_label]
    print(f"üéß Predicted Emotion: {predicted_label}")
    print(f"üß† Predicted Personality (OCEAN): {predicted_traits}")

    # Compare with actual if provided
    if actual_label:
        actual_traits = label_map.get(actual_label, None)
        print(f"‚úÖ Actual Emotion Label: {actual_label}")
        print(f"üß© Ground Truth Personality (OCEAN): {actual_traits}")

        # Radar comparison
        fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
        labels = ["O", "C", "E", "A", "N"]
        angles = np.linspace(0, 2 * np.pi, len(labels) + 1)

        pred_vals = predicted_traits + [predicted_traits[0]]
        ax.plot(angles, pred_vals, label="Predicted", color='blue')
        ax.fill(angles, pred_vals, alpha=0.2, color='blue')

        if actual_traits:
            act_vals = actual_traits + [actual_traits[0]]
            ax.plot(angles, act_vals, label="Actual", color='green')
            ax.fill(angles, act_vals, alpha=0.2, color='green')

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels)
        ax.set_title("Personality Radar Chart: Predicted vs Actual")
        ax.legend(loc="upper right")
        plt.tight_layout()

        # Save and export
        output_path = "audio_personality_report_with_comparison.pdf"
        generate_pdf(predicted_traits, fig, output_path=output_path)
        print(f"üìÑ PDF saved to: {output_path}")
    else:
        # No actual label provided, plot single radar
        fig = plot_radar(torch.tensor(predicted_traits), show=False)
        generate_pdf(predicted_traits, fig, output_path="audio_personality_report.pdf")

# Sample
sample_audio_path, actual_emotion = val_set[0]  # From your validation split

predict_personality_from_audio(
    audio_path=sample_audio_path,
    model=model,
    transform=transform,
    label_map=label_to_personality,
    actual_label=actual_emotion
)

from final_predict_personality_cpu import predict_personality_for_val_set

summary = predict_personality_for_val_set(
    val_set=val_set,
    model=model,
    transform=transform,
    label_map=label_to_personality,
    TARGET_EMOTIONS=['hap', 'ang', 'sad', 'neu']
)

import shutil

shutil.make_archive("val_reports", 'zip', "val_reports")

import pandas as pd
summary_df = pd.read_csv("val_reports/summary.csv")

#compute accuracy
correct = (summary_df["true_label"] == summary_df["predicted_label"]).sum()
total = len(summary_df)
accuracy = correct / total

print(f"Emotion Prediction Accuracy: {accuracy * 100:.2f}%")

