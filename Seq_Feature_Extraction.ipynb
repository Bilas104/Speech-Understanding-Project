{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cbyP7eATc6_D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "# Upload kaggle.json manually or mount from Google Drive\n",
        "kaggle_token = '/content/kaggle.json'\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp {kaggle_token} ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d sangayb/iemocap\n",
        "with zipfile.ZipFile(\"iemocap.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"iemocap_data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFYuq8lidLXf",
        "outputId": "4fdfb041-a961-443c-9386-43e55d98cd70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sangayb/iemocap\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import librosa\n",
        "\n",
        "# === Updated Feature Extraction: [T, D] MFCC + Delta\n",
        "def extract_features(y, sr=16000):\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "    stacked = np.vstack([mfcc, delta, delta2])  # [39, T]\n",
        "    return stacked.T  # [T, 39]\n",
        "\n",
        "# === Audio Loader\n",
        "def load_audio(file_path, sr=16000):\n",
        "    y, _ = librosa.load(file_path, sr=sr)\n",
        "    y = librosa.util.normalize(y)\n",
        "    return y\n",
        "\n",
        "# === Label Dictionary\n",
        "def build_label_dict(eval_dir):\n",
        "    label_dict = {}\n",
        "    for root, _, files in os.walk(eval_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".txt\"):\n",
        "                with open(os.path.join(root, file)) as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) >= 5 and parts[4] != 'xxx':\n",
        "                            utt_id = parts[3]\n",
        "                            label = parts[4]\n",
        "                            label_dict[utt_id] = label\n",
        "    return label_dict\n",
        "\n",
        "# === Dataset Loader with Padding\n",
        "def load_dataset(audio_root, label_dict, max_len=300):\n",
        "    X, y, speakers = [], [], []\n",
        "    for subdir, _, files in os.walk(audio_root):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\") and not file.startswith(\"._\"):\n",
        "                path = os.path.join(subdir, file)\n",
        "                utt_id = file.split(\".\")[0]\n",
        "                label = label_dict.get(utt_id)\n",
        "                if label:\n",
        "                    try:\n",
        "                        y_audio = load_audio(path)\n",
        "                        features = extract_features(y_audio)  # [T, D]\n",
        "\n",
        "                        # Pad or truncate to max_len\n",
        "                        if features.shape[0] < max_len:\n",
        "                            pad = np.zeros((max_len - features.shape[0], features.shape[1]))\n",
        "                            features = np.vstack([features, pad])\n",
        "                        else:\n",
        "                            features = features[:max_len]\n",
        "\n",
        "                        speaker_id = utt_id.split(\"_\")[0]\n",
        "                        X.append(features)\n",
        "                        y.append(label)\n",
        "                        speakers.append(speaker_id)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {file}: {e}\")\n",
        "    return X, y, speakers"
      ],
      "metadata": {
        "id": "lc4bAE0_dXk8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "dataset_root = \"iemocap_data\"\n",
        "sessions = [\"Session1\", \"Session2\", \"Session3\", \"Session4\", \"Session5\"]\n",
        "FEATURE_DIR = \"features_seq\"\n",
        "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
        "\n",
        "# Build Label Dictionary\n",
        "label_dict = {}\n",
        "for session in sessions:\n",
        "    eval_path = os.path.join(dataset_root, session, \"dialog\", \"EmoEvaluation\")\n",
        "    session_labels = build_label_dict(eval_path)\n",
        "    label_dict.update(session_labels)\n",
        "\n",
        "print(f\"Total labeled utterances: {len(label_dict)}\")\n",
        "\n",
        "# Extract and Process Features\n",
        "X, y, speakers = [], [], []\n",
        "for session in sessions:\n",
        "    audio_path = os.path.join(dataset_root, session, \"sentences\", \"wav\")\n",
        "    Xi, yi, si = load_dataset(audio_path, label_dict, max_len=300)\n",
        "    X.extend(Xi)\n",
        "    y.extend(yi)\n",
        "    speakers.extend(si)\n",
        "\n",
        "X = np.array(X)         # shape: [N, T, D]\n",
        "y = np.array(y)\n",
        "speakers = np.array(speakers)\n",
        "\n",
        "# Save .npy Files\n",
        "np.save(os.path.join(FEATURE_DIR, \"X_seq.npy\"), X)\n",
        "np.save(os.path.join(FEATURE_DIR, \"y_seq.npy\"), y)\n",
        "np.save(os.path.join(FEATURE_DIR, \"speakers_seq.npy\"), speakers)\n",
        "\n",
        "print(f\"Saved: X_seq.npy shape = {X.shape}, y_seq.npy, speakers_seq.npy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MpZpyLWfkMu",
        "outputId": "16264bca-f2e9-4af5-c2ba-64f585ab899f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total labeled utterances: 7548\n",
            "Saved: X_seq.npy shape = (7532, 300, 39), y_seq.npy, speakers_seq.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shWgex0tgUcn",
        "outputId": "bcc31c17-f0bf-4147-e9ea-1457110b4121"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/features_seq /content/drive/MyDrive/SU_Project/"
      ],
      "metadata": {
        "id": "nnE82GWngnJj"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}